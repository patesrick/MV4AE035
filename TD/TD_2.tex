% !TEX encoding = UTF-8 Unicode
\documentclass{../headers/td_upc}
\input{../headers/header_td.tex}

\def\version{eno}
%\def\version{cor}

\usepackage{hyperref}
\ue{MV4AE035}

\providecommand{\T}{\mathbb{T}}
\providecommand{\1}{\mathds{1}}
\title{TD 2 : Régression linéaire multiple}


\newcommand{\miniscule}{\@setfontsize\miniscule{5}{6}}
%-----------------------------------------------------------------------------
\begin{document}
	\maketitle
	
	
	\exo{Rappels de cours.}
	
	\begin{enumerate}
		\item Rappeler le principe d'une régression linéaire multiple. Préciser les hypothèses.
		\item Faire un schéma pour donner une interprétation géométrique à la régression linéaire multiple. Retrouvez l'expression de l'estimateur des moindres carrés $\hat \beta$.
		\item Donner l'expression de la matrice de projection $\mathbf{P}^{\mathbf{X}}$ et de l'estimateur $\hat \beta$. Vérifier que $\mathbf{P}^{\mathbf{X}}$ est bien une matrice de projection.
		\item Quelles sont les hypothèses supplémentaires dans le cas gaussien ?
		\item Dans le cas Gaussien, retrouvez l'expression des estimateurs du maximum de vraisemblance pour $\beta$ et $\sigma^2$ en annulant le gradient de la fonction à maximiser.
		\item Dans le cas Gaussien, retrouvez la loi de $\hat \beta$ et $\hat \sigma^2$, à variance connue ou inconnue.
	\end{enumerate}
	\textit{On conseille de toujours faire attention à la dimension des objets (matrices et vecteurs) qu'on manipule.}
	
	
	\exo{Régression simple vs régression multiple.}
	\begin{enumerate}
		\item Rappeler les expressions de $\hat \beta_0$ et $\hat \beta_1$ dans le cas d'une régression simple.
		\item Rappeler l'expression de $\hat \beta$ dans le cas d'une régression multiple.
		\item Retrouver le résultat de la question 1 à partir de celui de la question 2.
		\item Rappeler les expressions des variances et covariance de $\hat \beta_0$ et $\hat \beta_1$ pour une régression simple.
		\item Rappeler l'expression de la matrice de variance-covariance de $\hat \beta$ pour une régression multiple.
		\item Retrouver le résultat de la question 4 à partir de celui de la question 5.
	\end{enumerate}

	\cor{\newpage}

	\exo{Régression à deux variables.} On étudie l'évolution d'une variable $y$ en fonction de deux variables $x$ et $z$.
	On dispose de $n$ observations de ces variables.
	On note $X=\begin{pmatrix}\1 & x & z\end{pmatrix}$,
	où $\1$ est le vecteur constant, et $x$ et $z$ sont les vecteurs des variables explicatives.
	Nous avons obtenu les résultats suivants:
			\[
			X^T X=\begin{pmatrix}
				30 & 0 & 0 \\
				? & 10 & 7 \\
				? & ? & 15
			\end{pmatrix}
			\quad , \quad
			\|\hat{\varepsilon}\|^{2} = 12
				\quad , \quad
			\hat{\beta} = \begin{pmatrix} -2 \\ 1 \\ 2 \end{pmatrix}\,.
			\]
			\begin{enumerate}
			\item
			\begin{enumerate}
				\item Donner les valeurs manquantes. Que vaut $n$ ?
				\cor{
				Par symétrie,
							\[
			X^T X=\begin{pmatrix}
				30 & 0 & 0 \\
				0 & 10 & 7 \\
				0 & 7 & 15
			\end{pmatrix}\,.
			\]
			De plus, $n = \1^T \1$ est la première entrée $[X^T X]_{11}$ de la matrice,
			donc $n = 30$.
				}
				\item Calculer le coefficient de corrélation empirique entre $x$ et $z$.
				    \cor{
    On a:
    \begin{align*}
        n &= \1^T\1 =  [X^T X]_{11} = 30
        & n\bar{x} &= \1^Tx = [X^T X]_{12} = 0
        & n\bar{z} &= \1^Tz = [X^T X]_{13} = 0 \\
        x^Tx &= [X^T X]_{22} = 10
        & z^Tz &= [X^T X]_{33} = 15
        & x^Tz &= [X^T X]_{23} = 7
    \end{align*}
    Donc:
    \begin{align*}
        \rho(x, z)
        &= \frac{\sum_{i=1}^n (x_i - \bar{x})(z_i - \bar{z})}{\sqrt{\left(\sum_{i=1}^n (x_i - \bar{x})^2\right)\left(\sum_{i=1}^n (z_i - \bar{z})^2\right)}}
        = \frac{x^Tz - n\bar{x}\bar{z}}{\sqrt{(x^Tx - n\bar{x}^2)(z^Tz - n\bar{z}^2)}} \\
        &= \frac{7 - 0}{\sqrt{(10 - 0)(15 - 0)}} 
        = 0.57.
    \end{align*}
    }
			\end{enumerate}
			\item 
			\begin{enumerate}
				\item Calculer $\sum_{i=1}^n \hat \varepsilon_i$, puis en déduire la valeur de la moyenne arithmétique $\bar{y}$.
				\cor{
				Comme le vecteur $\1$ est dans l'espace engendré par les colonnes de $X$,
				on a :
				\[
				\sum_{i=1}^n \hat \varepsilon_i
				= \1^T \varepsilon = \langle \1, \varepsilon \rangle
				= \langle \1, P^{X^\bot} y \rangle
				= 0.
				\]
				Donc :
				\[
				\1^T \varepsilon = \1^T (y - \hat y) = 0 \quad \text{i.e.} \quad \bar y = \frac{1}{n} \1^T\hat y
				\]
				et
				\[
				\1^T\hat y = \1^T(X \hat \beta) 
				= \1^T (-2 \1 + x + 2 z)
				= -2 \times n + 0 + 2 \times 0.
				\]
				D'où $\bar{y} = -2$.
				}
				\item Calculer la somme des carrés résiduels (SCR),
				la somme des carrés expliquée (SCE),
				la somme des carrés totale (SCT) et le coefficient de détermination $R^{2}$.
				\cor{
				Par définition,
				\[
				SCR = \|\hat{\varepsilon}\|^{2} = 12.
				\]
				D'après ce qui précède, $\hat y - \1 \bar y = x + 2 z$ donc :
				\[
				SCE 
				= \|\hat y - \1 \bar y \|^{2}
				= \|x + 2 z \|^{2}
				= \|x\|^{2} + 4 \| z \|^{2} + 4 x^T z
				= 10 + 4 \times 15 + 4 \times 7
				= 98.
				\]
				Enfin, par le théorème de Pythagore :
				\[
				SCT = \|y - \1 \bar y \|^{2} 
				=  \|\hat y - \1 \bar y \|^{2} + \|\hat y - y\|^{2}
				= SCE + SCR
				= 98 + 12 = 110.
				\]
				Finalement :
				\[
				R^2 = \frac{SCE}{SCT} = \frac{98}{110} = 0.89.
				\]
				}
			\end{enumerate}
		\item 
		\begin{enumerate}
			\item Calculer $X^T y$ en utilisant la valeur de $\hat{\beta}$,
			puis en déduire $\sum x_{i} y_{i}$ et $\sum z_{i} y_{i}$.
			\cor{
			On a : $\hat \beta = (X^T X)^{-1} X^T y$, donc
			\[
			X^T y 
			= (X^T X) \hat \beta
			= 
			\begin{pmatrix}
				30 & 0 & 0 \\
				0 & 10 & 7 \\
				0 & 7 & 15
			\end{pmatrix}
			\begin{pmatrix} -2 \\ 1 \\ 2 \end{pmatrix}
			=\begin{pmatrix} -60 \\ 24 \\ 37 \end{pmatrix}.
			\]
			Donc :
			\[
			\sum x_{i} y_{i} = x^T y = [X^T y]_{2} = 24
			\quad \text{et} \quad
			\sum z_{i} y_{i} = z^T y = [X^T y]_{3} = 37.
			\]
			}
			\item Calculer les coefficients de corrélation $\rho_{x,y}$ et $\rho_{z,y}$.
			En déduire la valeur du $R^{2}$ pour le modèle de régression de $y$ par $\1 $ et $x$,
			puis de $y$ par $\1 $ et $z$.
			\cor{
			On a 
			\(
			y^Ty - n\bar{y}^2 = \sum_{i=1}^n (y_i - \bar{y}_i)^2 = SCT = 110
			\), donc :
			\begin{align*}
        \rho(x, y)
        & = \frac{x^Ty - n\bar{x}\bar{y}}{\sqrt{(x^Tx - n\bar{x}^2)(y^Ty - n\bar{y}^2)}}
        = \frac{24 - 0}{\sqrt{(10 - 0)(110)}} 
        = 0.72 ; \\
        \rho(z, y)
        & = \frac{z^Ty - n\bar{z}\bar{y}}{\sqrt{(z^Tz - n\bar{z}^2)(y^Ty - n\bar{y}^2)}}
        = \frac{37 - 0}{\sqrt{(15 - 0)(110)}} 
        = 0.91 ;
    \end{align*}
    Donc les $R^2$ sont donnés par:
    \[
    R^2_{\1,x} = \rho(x, y)^2 = 0.52
    \quad \text{and}\quad
    R^2_{\1,z} = \rho(z, y)^2 = 0.83.
    \]
			}
		\end{enumerate}
				\item 
			\begin{enumerate}
			\item Sous l'hypothèse gaussienne, donnez la loi de $\hat{\beta}_2$ le coefficient associé à $x$
			en fonction de $\beta_2$ et $\sigma^2$.
			\cor{
			\[
			\hat{\beta}_2 \sim \mathcal{N}(\beta_2, \sigma^2 [(X^T X)^{-1}]_{22}).
			\]
			}
			\item Calculer $(X^T X)^{-1}$.
			\cor{
			$(X^T X)^{-1}$ est une matrice diagonale par blocs, on peut donc
			l'inverser en prenant l'inverse du premier bloc $(30)^{-1} = 1/30$,
			et du second
			\[
				 \begin{pmatrix}
				10 & 7 \\
				7 & 15
			\end{pmatrix}^{-1}
			=
			\frac{1}{101}
			\begin{pmatrix}
				15 & -7 \\
				-7 & 10
			\end{pmatrix}
				\]
			Donc
				\[
				(X^T X)^{-1} \approx \begin{pmatrix}
				0.033 & 0 & 0 \\
				0 & 0.15 & -0.69 \\
				0 & -0.69 & 0.1
			\end{pmatrix}\,.
				\]
			}
			\item Calculez un estimateur sans biais de la variance $\sigma^2$.
			\cor{
			\[
			\hat{\sigma}^2 = \frac{\|\hat{\varepsilon}\|^{2}}{n-3} = \frac{12}{30-3} = 0.44.
			\]
			}
			\item Proposez un intervalle de confiance à $95\%$ pour $\beta_2$.
			Que peut-on conclure quant à la nullité de ce coefficient ?
			On donne le quantile à $97.5\%$ de la loi de Student à 27 degrés de liberté :
			$t_{27}(0.975) = 2.05$.
			\cor{
			On a :
			\[
			\frac{\hat{\beta}_2 - \beta_2}{\sqrt{ \hat{\sigma}^2 [(X^T X)^{-1}]_{22} }}
			= 
			\frac{\hat{\beta}_2 - \beta_2}{\sqrt{ 0.44 \times 0.15 }}
			=
			\frac{\beta_2 - 1}{0.26}
			\sim \mathcal{T}_{n-3}
			\]
			Donc, à $95\%$, $\beta_2$ est dans l'intervalle :
			\[
			[1 - t_{27}(0.975) \times 0.26 , 1 + t_{27}(0.975) \times 0.26]
			=
			[1 - 2.05 \times 0.26 ; 1 + 2.05 \times 0.26]
			= 
			[0.46; 1.53].
			\]
			Cet intervalle exclut zéro.
			}
			\end{enumerate}
	\end{enumerate}

% \cor{\newpage}
	\newpage
	\exo{Interprétation géométrique.}
	\begin{enumerate}
		\item Nous avons une variable $y$ à expliquer par une variable $x$. Nous avons effectué $n=2$ mesures et trouvé
		\[
		\left(x_{1}, y_{1}\right)=(4,5) \text { et }\left(x_{2}, y_{2}\right)=(1,5)
		\]
		Représenter les variables, estimer $\beta$ dans le modèle $y_{i}=\beta x_{i}+\varepsilon_{i}$ et représenter $\hat{y}$.
		\cor{
		On note $x = (4, 1)$ et $y = (5, 5)$.
		Alors $x^T x = 16 + 1 = 17$, et $x^T y = 20 + 5 = 25$, donc :
		\[
		\hat{\beta} = (x^Tx)^{-1} x^T y = 25 / 17 = 1.47
		\quad \text{et}\quad
		\hat y = x \hat{\beta} = (5.88, 1.47).
		\]
		}
		\item Nous avons maintenant une variable $y$ à expliquer par deux variables $x_{1}$ et $x_{2}$. Nous avons effectué $n=3$ mesures et trouvé
		\[
		(x_{1,1}, x_{1,2},y_{1})=(3,2,0),\,\quad
		(x_{2,1}, x_{2,2}, y_{2})=(3,3,5),\,\quad
		(x_{3,1}, x_{3,2}, y_{3})=(0,0,3)\,.
		\]
		Représenter les variables, estimer $\beta$ dans le modèle $y_{i}=\beta_{1} x_{i, 1}+\beta_{2} x_{i, 2}+\varepsilon_{i}$
		et représenter $\hat{y}$.
		\cor{
		On note 
		\[
		X = \begin{pmatrix} 3 & 2 \\ 3 & 3 \\ 0 & 0 \end{pmatrix}
		\quad \text{et}\quad
		Y = \begin{pmatrix} 0 \\ 5 \\ 3 \end{pmatrix}
		\]
		Alors :
		\[
		X^T X = \begin{pmatrix} 18 & 15 \\ 15 & 13\end{pmatrix}
		\quad\text{et}\quad
		(X^T X)^{-1} 
		= \frac{1}{9} \begin{pmatrix} 13 & - 15 \\ - 15 & 18\end{pmatrix}
		= \begin{pmatrix} 13 / 9 & - 5/3 \\ - 5/3 & 2\end{pmatrix}
		\]
		Finalement :
		\[
		X^T Y = \begin{pmatrix} 15 \\ 15 \end{pmatrix}
		\quad\text{et}\quad
		\hat{\beta} 
		= (X^T X)^{-1} X^T Y
		= \begin{pmatrix} -10 / 3 \\ 5 \end{pmatrix}
		\quad\text{et}\quad
		\hat{y} 
		= X (X^T X)^{-1} X^T Y
		= \begin{pmatrix} 0 \\ 5 \\ 0 \end{pmatrix}.
		\]
		}
	\end{enumerate}
	
	\cor{\newpage}
	
	\exo{Croissance de $R^2$.} Soit $X$ une matrice de taille $n \times p$ composée de $p$ vecteurs indépendants de $\mathbb{R}^{n}$.
	Nous notons $X_q$ la matrice composée des $q < p$ premiers vecteurs de $X$.
	On suppose que la première colone de $X$ est égale à $\1$, i.e. que l'intercept est inclu dans les deux modèles.
	Nous avons les deux modèles suivants :
	\[
	(1) \quad Y=X \beta+\varepsilon \quad \text { et } \quad (2) \quad Y=X_q \beta_q +\varepsilon_q\,.
	\]
	Comparer les $R^{2}$ dans les deux modèles.

\cor{
On note $P_p$ et $P_q$ les matrices de projection sur
les colones engendrées par $X$ et $X_q$,
et $P_p^\bot$, $P_q^\bot$ les projections sur leurs espaces orthogonaux.
Par hypothèse, $\text{Im}(X_q) \subset \text{Im}(X)$, donc $P_q P_p = P_q$.
On écrit:
\begin{align*}
\hat Y = P_p Y 
= (P_q + P_q^\bot) P_p Y  
= P_qP_p Y + P_q^\bot P_p Y 
= P_q Y + P_q^\bot P_p Y
= \hat Y_q + P_q^\bot P_p Y
\end{align*}
Donc par Pythagore, comme 
\[
\| \hat Y - \1 \bar Y\|^2
= \| P_q Y - \1 \bar Y + P_q^\bot P_p Y\|^2
= \| P_q Y - \1 \bar Y\|^2 + \| P_q^\bot P_p Y\|^2
\geq \| P_q Y - \1 \bar Y\|^2.
\]
Donc :
\[
R^2 
= \frac{\| \hat Y - \1 \bar Y\|^2}{\| Y - \1 \bar Y\|^2}
\geq \frac{\| \hat Y_q - \1 \bar Y\|^2}{\| Y - \1 \bar Y\|^2}
= R^2_q.
\]
Un modèle plus gros a toujours un $R^2$ plus grand.
}
	
	\cor{\newpage}
	
	\exo{Régression sur données agrégées par groupes} On suppose le modèle de régression
	\[
	Y = X \beta + \varepsilon\,,
	\quad \text{avec} \quad
	\mathbb{E}[\varepsilon]=0
	\quad \text{et} \quad \text{Var}(\varepsilon)=\sigma^{2} I_{n}\,.
	\]
	Les données individuelles $\left(x_{i 1}, \ldots, x_{i p}, y_{i}\right)$ ne sont cependant pas disponibles.
	On observe seulement les moyennes sur $I$ groupes, notés $C_{1}, \ldots, C_{I},$
	d'effectifs $n_{1}, \ldots, n_{I}$ :
	\[
	\bar{y}_{k}=\frac{1}{n_{k}} \sum_{i \in C_{k}} y_{i}
	\quad \text { et } \quad
	\bar{x}_{k j}=\frac{1}{n_{k}} \sum_{i \in C_{k}} x_{i j}\,.
	\]
	En notant $\bar{\varepsilon}_{k}=\frac{1}{n_{k}} \sum_{i \in C_{k}} \varepsilon_{i},$
	on a alors $\bar{Y}=\bar{X} \beta+\bar{\varepsilon}$.
	\begin{enumerate}
		\item Calculer $\mathbb{E}[\bar{\varepsilon}]$ et $\text{Var}[\bar{\varepsilon}]$.
		\cor{
		Pour tout $1 \leq k \leq I$, on a:
		\[
		\mathbb{E}[\bar{\varepsilon}_k]
		= \mathbb{E}\left[\frac{1}{n_{k}} \sum_{i \in C_{k}} \epsilon_{i}\right]
		= \frac{1}{n_{k}} \sum_{i \in C_{k}} \mathbb{E}[\epsilon_{i}]
		= 0
		\]
		donc $\mathbb{E}[\bar{\varepsilon}] = 0_I$.
		Pour tout $1 \leq k, \ell \leq I$, on a :
		\[
		\text{Cov}[\bar{\varepsilon}_k; \bar{\varepsilon}_\ell]
		= \text{Cov}\left[\frac{1}{n_{k}} \sum_{i \in C_{k}} \epsilon_{i}; \frac{1}{n_{\ell}} \sum_{j \in C_{\ell}} \epsilon_{j}\right]
		= \frac{1}{n_{k}n_{\ell}} \sum_{i \in C_{k}} \sum_{j \in C_{\ell}}\text{Cov}\left[ \epsilon_{i}; \epsilon_{j}\right]
		\]
		Si $k \neq \ell$, alors $\text{Cov}\left[ \epsilon_{i}; \epsilon_{j}\right] = 0$ 
		pour tout $i \in C_{k}$, $j \in C_{\ell}$, donc $\text{Cov}[\bar{\varepsilon}_k; \bar{\varepsilon}_\ell] = 0$.
		Si $k = \ell$, seuls les termes digonaux sont non nuls, et on obtient:
		\[
		\text{Var}[\bar{\varepsilon}_k]
		= \frac{1}{n_{k}^2} \sum_{i \in C_{k}} \text{Cov}\left[ \epsilon_{i}; \epsilon_{i}\right]
		=\frac{1}{n_{k}^2} \sum_{i \in C_{k}} \sigma^2
		=\frac{\sigma^2}{n_{k}}.
		\]
		Finalement:
		\[
		\text{Var}(\bar{\varepsilon}) = \sigma^2 
		\begin{pmatrix}
		1 / n_1 & 0 & \cdots & 0 \\
		0 & 1/n_2 & \cdots & 0 \\
		0 & 0 & \ddots &  0 \\
		0 & \cdots & 0 &  1 / n_I
		\end{pmatrix}.
		\]
		}
		\item On pose
		\[
		M = \operatorname{diag}(\sqrt{n_{1}}, \ldots, \sqrt{n_{I}})\,,
		\quad
		Y^{*} = M \bar{Y}\,,
		\quad
		X^{*} = M \bar{X}\,,
		\quad
		\varepsilon^{*}=M \bar{\varepsilon}\,.
		\]
		Quelle est la relation entre $Y^{*}$, $X^{*}$ et $\varepsilon^{*}$ ?
		Calculer $\mathbb{E}[\varepsilon^{*}]$ et $\text{Var}(\varepsilon^{*})$.
		\cor{
		En multipliant à gauche par $M$, on a:
		\[
		\bar{Y} =\bar{X} \beta+\bar{\varepsilon}
		\quad \implies \quad
		M \bar{Y} = M \bar{X} \beta + M \bar{\varepsilon}
		\quad \implies \quad
		Y^{*} = X^{*} \beta + \varepsilon^{*}.
		\]
		Et :
		\[
		\mathbb{E}[\varepsilon^{*}]
		= \mathbb{E}[M \bar{\varepsilon}]
		= M \mathbb{E}[\bar{\varepsilon}]
		= 0_I.
		\]
		Puis:
		\[
		\text{Var}[\varepsilon^{*}]
		= \text{Var}[M \bar{\varepsilon}]
		= M \text{Var}[\bar{\varepsilon}] M^T
		= \sigma^2 I_I.
		\]
		}
		\item En déduire un estimateur de $\beta$.
		\cor{
		On applique la formule au modèle en $*$ :
		\begin{align*}
		\hat \beta 
		&= ((X^{*})^T X^{*})^{-1} (X^{*})^T Y^{*} \\
		&= ((M \bar{X})^T M \bar{X})^{-1} (M \bar{X})^T M \bar{Y} \\
		&= (\bar{X}^T M^T M \bar{X})^{-1} \bar{X} M^T M \bar{Y} \\
		&= (\bar{X}^T M^2 \bar{X})^{-1} \bar{X} M^2 \bar{Y}.
		\end{align*}
		}
		\item Application numérique $: I=3$ avec $n_{1}=1$ et $n_{2}=n_{3}=2$.
		$\bar{X}_{1}^T=(1,1,1), \bar{X}_{2}^T=(7,12,5)$ et $\bar{Y}^T=(15,25,10)$.
		\cor{
		\[
		\bar X = \begin{pmatrix} 1 & 7 \\ 1 & 12 \\ 1 & 5 \end{pmatrix}
		\;,\quad
		M^2 = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 2 \end{pmatrix}
		\;,\quad
		M^2 \bar X = \begin{pmatrix} 1 & 7 \\ 2 & 24 \\ 2 & 10 \end{pmatrix}
		\;,\quad
		\bar{X}^T M^2 \bar X = \begin{pmatrix} 5 & 41 \\ 41 & 387 \end{pmatrix}
		\]
		\[
		(\bar{X}^T M^2 \bar X)^{-1} 
		= \frac{1}{254} \begin{pmatrix} 387 & -41 \\ -41 & 5 \end{pmatrix}
		\;,\quad
		\bar{X}^T M^2 \bar Y 
		= \begin{pmatrix} 1 & 2 & 2 \\ 7 & 24 & 10 \end{pmatrix}
		\begin{pmatrix} 15 \\ 25 \\ 10 \end{pmatrix}
		= \begin{pmatrix} 85 \\ 805 \end{pmatrix}
		\]
		\[
		\hat \beta
		= \frac{1}{254} \begin{pmatrix} 387 & -41 \\ -41 & 5 \end{pmatrix}
		\begin{pmatrix} 85 \\ 805 \end{pmatrix}
		= \frac{1}{254} \begin{pmatrix} -110 \\ 540 \end{pmatrix}
		\approx
		\begin{pmatrix} -0.43 \\ 2.13 \end{pmatrix}.
		\]
		Si on néglige l'hétérogénéité, i.e. $M = I_I$, on obtient:
		\[
		\hat \beta
		= \frac{1}{48} \begin{pmatrix} -20 \\ 165 \end{pmatrix}
		\approx
		\begin{pmatrix} -0.42 \\ 3.44 \end{pmatrix}.
		\]
		Si les classes ne sont pas équilibrées, cela conduit à une estimation
		biaisée de $\beta$.
		}
	\end{enumerate}
	
	\cor{\newpage}

	\exo{Estimateurs linéaires}  Soit $\theta_{1}$ et $\theta_{2}$ deux paramètres réels inconnus et soit :
	\begin{itemize}
			\item $Y_{1}$ un estimateur sans biais de $\theta_{1} + \theta_{2}$ et de variance $\sigma^{2}$
			\item $Y_{2}$ un estimateur sans biais de $2 \theta_{1} - \theta_{2}$ et de variance $4 \sigma^{2}$ 
			\item $Y_{3}$ un estimateur sans biais de $6 \theta_{1} + 3 \theta_{2}$ et de variance $9 \sigma^{2}$ 	
	\end{itemize}
		Les estimateurs $Y_{1}$, $Y_{2}$ et $Y_{3}$ étant indépendants, nous cherchons les estimateurs sans biais de $\theta_{1}$ et $\theta_{2}$, linéaires en $Y_{1}$, $Y_{2}$ et $Y_{3},$ et de variance minimale.
	\begin{enumerate}
		\item Notons $\tilde{\theta} = \alpha Y_{1} + \beta Y_{2} + \gamma Y_{3}$.
		\begin{enumerate}
			\item Quelles sont les équations à satisfaire pour que $\tilde{\theta}$ soit un estimateur sans biais de $\theta_{1} ?$
			\item Dans ce cas-là, exprimer la variance de $\tilde{\theta}$ et la minimiser.
			\item Idem pour $\theta_{2}$.
		\end{enumerate}
		\item On pose $Z_{1} = Y_{1}$, $Z_{2} = Y_{2} / 2$, et $Z_{3} = Y_{3} / 3$, et on note $Z=\left(Z_{1}, Z_{2}, Z_{3}\right)^T$ et $\theta=\left(\theta_{1}, \theta_{2}\right)^T$.
		\begin{enumerate}
			\item Trouver la matrice $X$ telle que $\mathbb{E}[Z]=X \theta$.
			\item Que vaut la matrice de variance-covariance de $Z$ ?
			\item On peut alors écrire $Z=X \theta+\varepsilon$. Retrouver les estimateurs de $\theta_{1}$ et $\theta_{2}$ calculés question 1.
		\end{enumerate}
	\end{enumerate}
	
		\cor{\newpage}

	\exo{Théorème de Gauss Markov}  L'objectif de cet exercice est de démontrer le 
	théorème de Gauss-Markov. On se place dans le cadre du modèle de régression multivarié
	classique (sans hypothèse gaussienne). Soit $\hat{\boldsymbol{\beta}}$ l'estimateur des moindres carrés
	du vecteur de coefficient $\boldsymbol{\beta}$. 
	\begin{enumerate}
		\item Rappellez les hypothèses du modèle de régression multiple, ainsi que la définition 
		de l'estimateur des moindres carrés, et donnez son expression.
		\cor{
		Voir le cour.
		$\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$.
		}
		\item Montrez que l'estimateur des moindres carrés $\hat \beta$ est linéaire et sans biais.
		Retrouvez l'expression de sa matrice de variance-covariance.
		\cor{
		Voir le cours.
		$\mathbb{E}[\hat{\boldsymbol{\beta}}] = \boldsymbol{\beta}$.
		$\mathbb{V}[\hat{\boldsymbol{\beta}}] = \sigma^2 (\mathbf{X}^T\mathbf{X})^{-1}$.
		}
		\item Soit $\tilde{\boldsymbol{\beta}}$ un autre estimateur linéaire sans biais de $\boldsymbol{\beta}$.
		\begin{enumerate}
			\item Montrez qu'il existe une matrice $\mathbf{B}$ déterministe telle que $\tilde{\boldsymbol{\beta}} = \mathbf{B}\mathbf{y}$.
			Précisez les dimensions de $\mathbf{B}$.
			\cor{
			Par définition, $\tilde{\boldsymbol{\beta}}$ est un estimateur linéaire, donc
			il existe $\mathbf{B}$ de taille $p \times n$ telle que
			$\tilde{\boldsymbol{\beta}} = \mathbf{B}\mathbf{y}$.
			}
			\item Montrez que, pour tout vecteur $\boldsymbol{\beta}$ de coefficient,
			$\mathbf{B} \mathbf{X} \boldsymbol{\beta} = \boldsymbol{\beta}$.
			En déduire que $\mathbf{B} \mathbf{X} = \mathbf{I}_p$ où $\mathbf{I}_p$ 
			est la matrice identité de taille $p$.
			\cor{
			$\tilde{\boldsymbol{\beta}}$ est un estimateur sans biais, donc,
			pour tout $\boldsymbol{\beta}$,
			$$
			\boldsymbol{\beta}
			= \mathbb{E}[\tilde{\boldsymbol{\beta}}] 
			= \mathbb{E}[\mathbf{B}\mathbf{y}] 
			= \mathbf{B}\mathbb{E}[\mathbf{y}] 
			= \mathbf{B} \mathbf{X} \boldsymbol{\beta}.
			$$
			}
		\end{enumerate}
		\item Soient $\mathbf{S}_1$ et $\mathbf{S}_2$ deux matrices symétriques réelles
		de taille $p \times p$.
		On dit que $\mathbf{S}_1 \leq \mathbf{S}_2$ si la matrice 
		$\mathbf{S}_2 - \mathbf{S}_1$ est une matrice symétrique positive,
		i.e. si elle est symétrique, et, pour tout $\mathbf{u} \in \mathbb{R}^p$,
		$\mathbf{u}^T (\mathbf{S}_2 - \mathbf{S}_1) \mathbf{u} \geq 0$.
		On cherche à montrer que 
		$\mathbb{V}[\hat{\boldsymbol{\beta}}] \leq \mathbb{V}[\tilde{\boldsymbol{\beta}}]$.
		\begin{enumerate}
			\item Montrez: 
$
\mathbb{V}[\tilde{\boldsymbol{\beta}}] - \mathbb{V}[\hat{\boldsymbol{\beta}}]
= 
\mathbb{V}[\tilde{\boldsymbol{\beta}} - \hat{\boldsymbol{\beta}}]
+ \mathbb{C}[\tilde{\boldsymbol{\beta}} - \hat{\boldsymbol{\beta}}; \hat{\boldsymbol{\beta}}] 
+ \mathbb{C}[\tilde{\boldsymbol{\beta}} - \hat{\boldsymbol{\beta}}; \hat{\boldsymbol{\beta}}]^T
$.
\cor{
$$
\begin{aligned}
\mathbb{V}[\tilde{\boldsymbol{\beta}}]
& = \mathbb{V}[\tilde{\boldsymbol{\beta}} - \hat{\boldsymbol{\beta}} + \hat{\boldsymbol{\beta}}]\\
& = 
\mathbb{V}[\tilde{\boldsymbol{\beta}} - \hat{\boldsymbol{\beta}}]
+ \mathbb{V}[\hat{\boldsymbol{\beta}}]
+ \mathbb{C}[\tilde{\boldsymbol{\beta}} - \hat{\boldsymbol{\beta}}; \hat{\boldsymbol{\beta}}] 
+ \mathbb{C}[\tilde{\boldsymbol{\beta}} - \hat{\boldsymbol{\beta}}; \hat{\boldsymbol{\beta}}]^T
\end{aligned}
$$
}
\item Montrez:
$
\mathbb{C}[\tilde{\boldsymbol{\beta}}; \hat{\boldsymbol{\beta}}]
= \sigma^2 (\mathbf{X}^T\mathbf{X})^{-1}.
$
\cor{
$$
\begin{aligned}
\mathbb{C}[\tilde{\boldsymbol{\beta}}; \hat{\boldsymbol{\beta}}]
& = \mathbb{C}[\mathbf{B} \mathbf{y}; (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}]
= \mathbf{B} \mathbb{C}[\mathbf{y}; \mathbf{y}] [(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T]^T\\
& = \mathbf{B} [\sigma^2 \mathbf{I}_n] \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}
= \sigma^2 \mathbf{B} \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}
= \sigma^2 (\mathbf{X}^T\mathbf{X})^{-1}
\end{aligned}
$$
}
\item Montrez:
$
\mathbb{C}[\tilde{\boldsymbol{\beta}} - \hat{\boldsymbol{\beta}}; \hat{\boldsymbol{\beta}}] = 0.
$
\cor{
$$
\begin{aligned}
\mathbb{C}[\tilde{\boldsymbol{\beta}} - \hat{\boldsymbol{\beta}}; \hat{\boldsymbol{\beta}}]
& = \mathbb{C}[\tilde{\boldsymbol{\beta}}; \hat{\boldsymbol{\beta}}] - \mathbb{V}[\hat{\boldsymbol{\beta}}]\\
& = \sigma^2 (\mathbf{X}^T\mathbf{X})^{-1} - \sigma^2 (\mathbf{X}^T\mathbf{X})^{-1}
= 0.
\end{aligned}
$$
}
\item En déduire que
$
\mathbb{V}[\tilde{\boldsymbol{\beta}}] - \mathbb{V}[\hat{\boldsymbol{\beta}}]
$ est une matrice symétrique positive.
\cor{
D'après ce qui précède:
$
\mathbb{V}[\tilde{\boldsymbol{\beta}}] - \mathbb{V}[\hat{\boldsymbol{\beta}}]
= 
\mathbb{V}[\tilde{\boldsymbol{\beta}} - \hat{\boldsymbol{\beta}}].
$
$\mathbb{V}[\tilde{\boldsymbol{\beta}}] - \mathbb{V}[\hat{\boldsymbol{\beta}}]$ 
est donc égale à une matrice de variance-covariance, et est donc une matrice 
symétrique positive.
}
	\end{enumerate}
	\item Conclure la démontration du théorème de Gauss-Markov.
	\cor{
	On a bien montré que $\hat{\boldsymbol{\beta}}$ était l'estimateur
	linéaire sans biais qui avait la plus petite matrice de variance-covariance
	au sens matriciel.
	}
	\end{enumerate}

\exo{Théorème de Cochran}  L'objectif de cet exercice est de démontrer la version du 
	théorème de Cochran utilisée dans le cours. 
	Soient :
	\begin{itemize}
	\item $\mathbf{Y}$ un vecteur gaussien $\mathbf{Y} \sim \mathcal{N}(\boldsymbol{\mu}, \sigma^2\mathbf{I}_n)$;
  \item $\mathcal{M}$ un sous-espace vectoriel de $\mathbb{R}^n$ de dimension $p$;
  \item $\mathbf{P}$ la matrice de projection orthogonale sur $\mathcal{M}$;
  \item $\mathbf{P}^{\bot} = \mathbf{I}_n - \mathbf{P}$ la matrice de la projection sur l'espace orthogonal $\mathcal{M}^{\bot}$.
	\end{itemize}
	On cherche alors à montrer les énoncés suivants :
	\begin{itemize}
	\item[(i)] $\mathbf{P}\mathbf{Y} \sim \mathcal{N}(\mathbf{P}\boldsymbol{\mu}, \sigma^2\mathbf{P})$
et
$\mathbf{P}^{\bot}\mathbf{Y} \sim \mathcal{N}(\mathbf{P}^{\bot}\boldsymbol{\mu}, \sigma^2\mathbf{P}^{\bot})$;
	\item[(ii)] $\mathbf{P}\mathbf{Y}$ et $\mathbf{P}^{\bot}\mathbf{Y}$ sont independents;
	\item[(iii)]
$\frac{1}{\sigma^2}\|\mathbf{P}(\mathbf{Y} - \boldsymbol{\mu}) \|^2 \sim \chi^2_{p}$
et
$\frac{1}{\sigma^2}\|\mathbf{P}^{\bot}(\mathbf{Y} - \boldsymbol{\mu}) \|^2 \sim \chi^2_{n - p}$.
	\end{itemize}
	Ce sont ces propriétés qui permettent de conclure sur la loi des estimateurs dans le
	cas de la régression linéaire gaussienne.

	\begin{enumerate}
		\item Montrez que, si l'on admet le théorème, on peut retrouver la loi des
		estimateurs $\hat{\boldsymbol{\beta}}$ et $\hat{\sigma}^2$ dans le cas classique
		du modèle gaussien multivarié.
		\item En utilisant les propriétés usuelles des vecteurs gaussiens, montrez l'énoncé (i).
		\item Soit $\mathbf{U}$ une matrice orthogonale de taille $n$ 
		(telle que $\mathbf{U}\mathbf{U}^T = \mathbf{I}_n$), 
		et $\mathbf{\Delta}$ une matrice diagonale vérifiant $\Delta_{ii} = 1$ si $1 \leq i \leq p$
		et $\Delta_{ii} = 0$ si $i \geq p$, telles que $\mathbf{P} = \mathbf{U}\mathbf{\Delta}\mathbf{U}^T$.
		Soit $\mathbf{Z} = \mathbf{U}^T\mathbf{Y}$. 
		\begin{enumerate}
		\item Justifiez l'existence de la décomposition $\mathbf{P} = \mathbf{U}\mathbf{\Delta}\mathbf{U}^T$.
		\item Donnez la loi du vecteur $\mathbf{Z}$.
		\item Montrez que $\mathbf{\Delta}\mathbf{Z}$ est indépendant de $(\mathbf{I_n} - \mathbf{\Delta})\mathbf{Z}$.
		\item Montrez que $\mathbf{U}\mathbf{\Delta}\mathbf{Z} = \mathbf{P} \mathbf{Y}$ 
		et $\mathbf{U}(\mathbf{I_n} - \mathbf{\Delta})\mathbf{Z} = \mathbf{P}^{\bot} \mathbf{Y}$.
		\item En déduire l'énoncé (ii).
		\end{enumerate}
		\item On cherche dans cette question à montrer le lemme intermédiaire suivant.
		Soit $\mathbf{X} \sim \mathcal{N}(\mathbf{m}, \mathbf{\Sigma})$
		une variable aléatoire de dimension $q$, avec $\mathbf{\Sigma}$ une matrice symétrique
		définie positive. Alors la variable $\rho = (\mathbf{X} - \mathbf{m})^T \mathbf{\Sigma}^{-1}(\mathbf{X} - \mathbf{m})$
		suit la loi $\chi^2_q$ du chi deux à $q$ degrés de libertés.
		\begin{enumerate}
		\item Montrez le lemme dans le cas $q = 1$.
		\item Dans le cas général, justifiez l'existence de $\mathbf{\Sigma}^{-1}$,
		et montrez qu'il existe une matrice $\mathbf{V}$ inversible telle 
		que $\mathbf{\Sigma}^{-1} = \mathbf{V}^T\mathbf{V}$.
		\item Donnez la loi de $\tilde{\mathbf{X}} = \mathbf{V}(\mathbf{X} - \mathbf{m})$.
		\item En déduire que $\|\tilde{\mathbf{X}}\|^2 \sim \chi^2_q$.
		\item Concluez la démonstration du lemme en montrant que 
		$\|\tilde{\mathbf{X}}\|^2 = (\mathbf{X} - \mathbf{m})^T \mathbf{\Sigma}^{-1}(\mathbf{X} - \mathbf{m})$.
		\end{enumerate}
		\item On s'interesse maintenant au terme quadratique de (iii).
		On note $\mathbf{Z}_p = (\mathbf{Z})_{1 \leq i \leq p}$ le vecteur contenant
		les $p$ premières coordonées du vecteur $\mathbf{Z}$ défini ci-dessus.
		\begin{enumerate}
		\item Montrez :
		$
		\|\mathbf{P}(\mathbf{Y} - \boldsymbol{\mu}) \|^2 = 
		(\mathbf{\Delta}\mathbf{U}^T\mathbf{Y} - \mathbf{\Delta}\mathbf{U}^T\boldsymbol{\mu})^T
		(\mathbf{\Delta}\mathbf{U}^T\mathbf{Y} - \mathbf{\Delta}\mathbf{U}^T\boldsymbol{\mu})
		$.
		\item 
		En déduire :
		$
		\|\mathbf{P}(\mathbf{Y} - \boldsymbol{\mu}) \|^2 = 
		(\mathbf{Z}_p - \mathbb{E}[\mathbf{Z}_p])^T(\mathbf{Z}_p - \mathbb{E}[\mathbf{Z}_p])
		$.
		\item En utilisant le lemme ci-dessus, en déduire que 
		$\frac{1}{\sigma^2}\|\mathbf{P}(\mathbf{Y} - \boldsymbol{\mu}) \|^2 \sim \chi^2_{p}$.
		\item Concluez la démonstration de l'énoncé (iii).
		\end{enumerate}
	\end{enumerate}
\end{document}



